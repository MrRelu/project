{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqF4O4Z4Vx0ZAqZ7DfQKhK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrRelu/project/blob/main/Possible_Overfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-8yiQ7OVtLJ",
        "outputId": "fec1bd15-665b-4222-9506-faec84b1e216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "dataset_path = kagglehub.dataset_download('muhammadqasimshabbir/amini-soil-prediction-challenge-dataset')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna fpdf xgboost catboost optuna-integration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Es78DrV66L",
        "outputId": "889b6e48-72d2-4fab-e2f7-1ebf9895b4bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting optuna-integration\n",
            "  Downloading optuna_integration-4.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna_integration-4.4.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=94fe5c0e2dc393210292cc69cb044648935820aba753556233e3af15291b0ad4\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf, colorlog, alembic, optuna, catboost, optuna-integration\n",
            "Successfully installed alembic-1.16.2 catboost-1.2.8 colorlog-6.9.0 fpdf-1.7.2 optuna-4.4.0 optuna-integration-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS8JmeyeWGvX",
        "outputId": "a1af3d81-ac85-4622-deea-dc583831ab58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_df = pd.read_csv(dataset_path+'/Train.csv')\n",
        "test_df = pd.read_csv(dataset_path+'/Test.csv')\n",
        "train_gap_df = pd.read_csv(dataset_path+'/Gap_Train.csv')\n",
        "test_gap_df1 = pd.read_csv(dataset_path+'/Gap_Test.csv')\n",
        "sample_submission = pd.read_csv(dataset_path+'/SampleSubmission.csv')\n",
        "test_gap_df = pd.merge(test_gap_df1, test_df[['PID', 'BulkDensity']], on='PID')"
      ],
      "metadata": {
        "id": "Ybq325cDWVv6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: input missing values in train_df and test_df with the mean, only do it for columns that have missing values\n",
        "\n",
        "# Fill missing values with the mean for columns with missing values in train_df\n",
        "for column in train_df.columns:\n",
        "  if train_df[column].isnull().any():\n",
        "    train_df[column].fillna(train_df[column].mean(), inplace=True)\n",
        "\n",
        "# Fill missing values with the mean for columns with missing values in test_df\n",
        "for column in test_df.columns:\n",
        "  if test_df[column].isnull().any():\n",
        "    test_df[column].fillna(test_df[column].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "ULILoeraWgMj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import optuna\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "# from optuna.integration import CatBoostPruningCallback\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from fpdf import FPDF\n",
        "\n",
        "output_path=\"/content/drive/MyDrive\"\n",
        "\n",
        "# Define targets and corresponding level columns\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# Prepare test set\n",
        "X_test = test_df.drop(columns=['PID', 'site'])\n",
        "test_ids = test_df['PID']\n",
        "\n",
        "# Define Optuna objective function\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "        \"iterations\": 1000,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 11),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
        "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
        "        \"loss_function\": \"RMSE\",\n",
        "        \"early_stopping_rounds\": 50,\n",
        "        \"random_seed\": 42,\n",
        "        \"verbose\": False,\n",
        "    }\n",
        "\n",
        "    train_pool = Pool(X_train, y_train)\n",
        "    val_pool = Pool(X_val, y_val)\n",
        "\n",
        "    model = CatBoostRegressor(**params,)\n",
        "\n",
        "    model.fit(train_pool, eval_set=val_pool)\n",
        "    preds = model.predict(val_pool)\n",
        "    return np.sqrt(mean_squared_error(y_val, preds))\n",
        "\n",
        "# Containers to track results\n",
        "final_predictions = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "best_params_dict = {}  # To store best parameters per target\n",
        "\n",
        "# Loop over each target variable\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Processing Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    # Split for tuning\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=stratify_vals, random_state=42\n",
        "    )\n",
        "\n",
        "    # Tune with Optuna\n",
        "    pruner = SuccessiveHalvingPruner()\n",
        "    study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "    for _ in tqdm(range(20), desc=f\"Tuning {target}\"):\n",
        "        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=1)\n",
        "\n",
        "    best_params = study.best_trial.params\n",
        "    print(f\"Best Parameters for {target}: {best_params}\")\n",
        "    best_params_dict[target] = best_params  # Save for export\n",
        "\n",
        "    # Cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        train_pool = Pool(X_tr, y_tr)\n",
        "        val_pool = Pool(X_vl, y_vl)\n",
        "\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=1000,\n",
        "            early_stopping_rounds=50,\n",
        "            loss_function=\"RMSE\",\n",
        "            random_seed=42,\n",
        "            verbose=False,\n",
        "            **best_params\n",
        "        )\n",
        "\n",
        "        model.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "        val_preds = model.predict(val_pool)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    final_predictions[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# Overall score\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# Save submission\n",
        "final_predictions.to_csv(output_path+\"/Cat_submission.csv\", index=False)\n",
        "\n",
        "# Save best parameters to pickle file\n",
        "with open(output_path+\"/best_catboost_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_params_dict, f)\n",
        "\n",
        "# Save best parameters to PDF\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "pdf.cell(200, 10, txt=\"Best CatBoost Parameters per Target\", ln=True, align=\"C\")\n",
        "pdf.ln(10)\n",
        "\n",
        "for target, params in best_params_dict.items():\n",
        "    pdf.set_font(\"Arial\", 'B', size=12)\n",
        "    pdf.cell(200, 10, txt=f\"{target}:\", ln=True)\n",
        "    pdf.set_font(\"Arial\", size=11)\n",
        "    for key, value in params.items():\n",
        "        pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\n",
        "    pdf.ln(5)\n",
        "\n",
        "pdf.output(output_path+\"/best_catboost_params.pdf\")\"\"\""
      ],
      "metadata": {
        "id": "4-oeXwBmXm3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "dir_with_params= './'\n",
        "\n",
        "with open(dir_with_params+\"best_catboost_params.pkl\", \"rb\") as f:\n",
        "    best_param_dict = pickle.load(f)\n",
        "# Containers to track results\n",
        "# Define targets and corresponding level columns\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# Prepare test set\n",
        "X_test = test_df.drop(columns=['PID', 'site',])\n",
        "test_ids = test_df['PID']\n",
        "CAT = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "best_params_dict = {}  # To store best parameters per target\n",
        "\n",
        "# Loop over each target variable\n",
        "for target in target_columns:\n",
        "    best_params=best_params_dict[target]\n",
        "    print(f\"\\n=== Processing Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    # Cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        train_pool = Pool(X_tr, y_tr)\n",
        "        val_pool = Pool(X_vl, y_vl)\n",
        "\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=1000,\n",
        "            early_stopping_rounds=50,\n",
        "            loss_function=\"RMSE\",\n",
        "            random_seed=42,\n",
        "            verbose=False,\n",
        "            **best_params\n",
        "        )\n",
        "\n",
        "        model.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "        val_preds = model.predict(val_pool)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    CAT[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# Overall score\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# Save submission\n",
        "CAT.to_csv(\"Cat_submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "DnKRtbF3WsMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import logging\n",
        "logging.getLogger(\"xgboost\").setLevel(logging.ERROR)\n",
        "\n",
        "# Define targets and corresponding level columns\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# Prepare test set\n",
        "X_test = test_df.drop(columns=['PID', 'site'])\n",
        "test_ids = test_df['PID']\n",
        "\n",
        "# Containers to track results\n",
        "final_predictions = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "best_params_dict = {}\n",
        "\n",
        "# Loop over each target variable\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Processing Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    # Split for tuning\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=stratify_vals, random_state=42\n",
        "    )\n",
        "\n",
        "    # Define Optuna objective\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"eval_metric\": \"rmse\",\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "            \"lambda\": trial.suggest_float(\"lambda\", 0.0, 5.0),\n",
        "            \"alpha\": trial.suggest_float(\"alpha\", 0.0, 5.0),\n",
        "            \"n_estimators\": 1000,\n",
        "            \"random_state\": 42,\n",
        "            \"early_stopping_rounds\":50\n",
        "\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            #callbacks=[\n",
        "               # XGBoostPruningCallback(trial, \"validation_0-rmse\"),\n",
        "                #xgb.callback.EarlyStopping(rounds=50)\n",
        "           # ],\n",
        "            verbose=False\n",
        "        )\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "        return rmse\n",
        "\n",
        "    # Run Optuna tuning\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    for _ in tqdm(range(20), desc=f\"Tuning {target}\"):\n",
        "        study.optimize(objective, n_trials=1)\n",
        "\n",
        "    best_params = study.best_trial.params\n",
        "    best_params.update({\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"eval_metric\": \"rmse\",\n",
        "        \"n_estimators\": 1000,\n",
        "        \"random_state\": 42,\n",
        "    })\n",
        "    print(f\"Best Parameters for {target}: {best_params}\")\n",
        "    best_params_dict[target] = best_params\n",
        "\n",
        "    # Cross-validation with best params\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = xgb.XGBRegressor(**best_params)\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_vl, y_vl)],\n",
        "           # callbacks=[\n",
        "               # xgb.callback.EarlyStopping(rounds=50)\n",
        "          #  ],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        val_preds = model.predict(X_vl)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    final_predictions[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# Overall score\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# Save submission\n",
        "final_predictions.to_csv(\"/content/drive/MyDrive/No_lon_XGb_submission.csv\", index=False)\n",
        "# Save best parameters to pickle file\n",
        "with open(\"/content/drive/MyDrive/best_xgb_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_params_dict, f)\n",
        "\n",
        "# Save best parameters to PDF\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "pdf.cell(200, 10, txt=\"Best XGBOOST Parameters per Target\", ln=True, align=\"C\")\n",
        "pdf.ln(10)\n",
        "\n",
        "for target, params in best_params_dict.items():\n",
        "    pdf.set_font(\"Arial\", 'B', size=12)\n",
        "    pdf.cell(200, 10, txt=f\"{target}:\", ln=True)\n",
        "    pdf.set_font(\"Arial\", size=11)\n",
        "    for key, value in params.items():\n",
        "        pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\n",
        "    pdf.ln(5)\n",
        "\n",
        "pdf.output(\"/content/drive/MyDrive/best_xgb_params.pdf\") \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "collapsed": true,
        "id": "qykCBiOzZEa1",
        "outputId": "4b310ed1-71c2-4539-c543-4f62abe3fcff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.metrics import mean_squared_error\\nimport xgboost as xgb\\nimport optuna\\nfrom optuna.integration import XGBoostPruningCallback\\nfrom tqdm import tqdm\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\nimport logging\\nlogging.getLogger(\"xgboost\").setLevel(logging.ERROR)\\n\\n# Define targets and corresponding level columns\\ntarget_columns = [\\'N\\', \\'P\\', \\'K\\', \\'Ca\\', \\'Mg\\', \\'S\\', \\'Fe\\', \\'Mn\\', \\'Zn\\', \\'Cu\\', \\'B\\']\\nlevel_columns = {target: f\"{target}_level\" for target in target_columns}\\n\\n# Prepare test set\\nX_test = test_df.drop(columns=[\\'PID\\', \\'site\\'])\\ntest_ids = test_df[\\'PID\\']\\n\\n# Containers to track results\\nfinal_predictions = pd.DataFrame({\\'PID\\': test_ids})\\ncv_scores = {}\\nbest_params_dict = {}\\n\\n# Loop over each target variable\\nfor target in target_columns:\\n    print(f\"\\n=== Processing Target: {target} ===\")\\n    level_col = level_columns[target]\\n\\n    drop_cols = target_columns + list(level_columns.values()) + [\\'PID\\', \\'site\\']\\n    X = train_df.drop(columns=drop_cols)\\n    y = train_df[target]\\n    stratify_vals = train_df[level_col]\\n\\n    # Split for tuning\\n    X_train, X_val, y_train, y_val = train_test_split(\\n        X, y, test_size=0.2, stratify=stratify_vals, random_state=42\\n    )\\n\\n    # Define Optuna objective\\n    def objective(trial):\\n        params = {\\n            \"objective\": \"reg:squarederror\",\\n            \"tree_method\": \"hist\",\\n            \"eval_metric\": \"rmse\",\\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\\n            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\\n            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\\n            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\\n            \"lambda\": trial.suggest_float(\"lambda\", 0.0, 5.0),\\n            \"alpha\": trial.suggest_float(\"alpha\", 0.0, 5.0),\\n            \"n_estimators\": 1000,\\n            \"random_state\": 42,\\n            \"early_stopping_rounds\":50\\n\\n        }\\n\\n        model = xgb.XGBRegressor(**params)\\n        model.fit(\\n            X_train, y_train,\\n            eval_set=[(X_val, y_val)],\\n            #callbacks=[\\n               # XGBoostPruningCallback(trial, \"validation_0-rmse\"),\\n                #xgb.callback.EarlyStopping(rounds=50)\\n           # ],\\n            verbose=False\\n        )\\n        preds = model.predict(X_val)\\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\\n        return rmse\\n\\n    # Run Optuna tuning\\n    study = optuna.create_study(direction=\"minimize\")\\n    for _ in tqdm(range(20), desc=f\"Tuning {target}\"):\\n        study.optimize(objective, n_trials=1)\\n\\n    best_params = study.best_trial.params\\n    best_params.update({\\n        \"objective\": \"reg:squarederror\",\\n        \"tree_method\": \"hist\",\\n        \"eval_metric\": \"rmse\",\\n        \"n_estimators\": 1000,\\n        \"random_state\": 42,\\n    })\\n    print(f\"Best Parameters for {target}: {best_params}\")\\n    best_params_dict[target] = best_params\\n\\n    # Cross-validation with best params\\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n    fold_rmses = []\\n    fold_preds = np.zeros(len(X_test))\\n\\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\\n        print(f\" Fold {fold}/5\")\\n        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\\n        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\\n\\n        model = xgb.XGBRegressor(**best_params)\\n        model.fit(\\n            X_tr, y_tr,\\n            eval_set=[(X_vl, y_vl)],\\n           # callbacks=[\\n               # xgb.callback.EarlyStopping(rounds=50)\\n          #  ],\\n            verbose=False\\n        )\\n\\n        val_preds = model.predict(X_vl)\\n        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\\n        fold_rmses.append(rmse)\\n        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\\n\\n        fold_preds += model.predict(X_test) / skf.n_splits\\n\\n    mean_rmse = np.mean(fold_rmses)\\n    cv_scores[target] = mean_rmse\\n    final_predictions[target] = fold_preds\\n    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\\n\\n# Overall score\\noverall_rmse = np.mean(list(cv_scores.values()))\\nprint(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\\n\\n# Save submission\\nfinal_predictions.to_csv(\"/content/drive/MyDrive/No_lon_XGb_submission.csv\", index=False)\\n# Save best parameters to pickle file\\nwith open(\"/content/drive/MyDrive/best_xgb_params.pkl\", \"wb\") as f:\\n    pickle.dump(best_params_dict, f)\\n\\n# Save best parameters to PDF\\npdf = FPDF()\\npdf.add_page()\\npdf.set_font(\"Arial\", size=12)\\npdf.cell(200, 10, txt=\"Best XGBOOST Parameters per Target\", ln=True, align=\"C\")\\npdf.ln(10)\\n\\nfor target, params in best_params_dict.items():\\n    pdf.set_font(\"Arial\", \\'B\\', size=12)\\n    pdf.cell(200, 10, txt=f\"{target}:\", ln=True)\\n    pdf.set_font(\"Arial\", size=11)\\n    for key, value in params.items():\\n        pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\\n    pdf.ln(5)\\n\\npdf.output(\"/content/drive/MyDrive/best_xgb_params.pdf\") '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "dir_with_params= './'\n",
        "\n",
        "# Load best parameters\n",
        "with open(dir_with_params+\"best_xgb_params.pkl\", \"rb\") as f:\n",
        "    best_params_dict = pickle.load(f)\n",
        "\n",
        "# Target columns and level mapping\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# Prepare test data\n",
        "X_test = test_df.drop(columns=['PID', 'site'])\n",
        "test_ids = test_df['PID']\n",
        "\n",
        "# Prepare output container\n",
        "XGB = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "\n",
        "# Loop over each target\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Retraining Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = xgb.XGBRegressor(**best_params_dict[target])\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_vl, y_vl)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        val_preds = model.predict(X_vl)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\" Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    XGB[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# Save predictions\n",
        "XGB.to_csv(\"/content/drive/MyDrive/Retrained_XGB_submission.csv\", index=False)\n",
        "\n",
        "# Show overall RMSE\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "S1lRvmSHcOd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from optuna.integration import LightGBMPruningCallback\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import logging\n",
        "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
        "\n",
        "# Define targets and corresponding level columns\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# Prepare test set\n",
        "X_test = test_df.drop(columns=['PID', 'site'])\n",
        "test_ids = test_df['PID']\n",
        "\n",
        "# Containers to track results\n",
        "final_predictions = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "best_params_dict = {}\n",
        "\n",
        "# Loop over each target variable\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Processing Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    # Split for tuning\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=stratify_vals, random_state=42\n",
        "    )\n",
        "\n",
        "    # Define Optuna objective\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 64),\n",
        "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 5),\n",
        "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100),\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\",300, 1200),\n",
        "            \"random_state\": 42,\n",
        "        }\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val,y_val)],\n",
        "            callbacks=[LightGBMPruningCallback(trial, \"l2\")],\n",
        "            #verbose=False\n",
        "        )\n",
        "        preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
        "        return rmse\n",
        "\n",
        "    # Run Optuna tuning\n",
        "    pruner = SuccessiveHalvingPruner()\n",
        "    study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "\n",
        "    for _ in tqdm(range(60), desc=f\"Tuning {target}\"):\n",
        "        study.optimize(objective, n_trials=2)\n",
        "\n",
        "    best_params = study.best_trial.params\n",
        "    best_params.update({\n",
        "        \"n_estimators\": 1000,\n",
        "        \"random_state\": 42,\n",
        "    })\n",
        "    print(f\"Best Parameters for {target}: {best_params}\")\n",
        "    best_params_dict[target] = best_params\n",
        "\n",
        "    # Cross-validation with best params\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**best_params)\n",
        "        model.fit(\n",
        "            X_tr,y_tr,\n",
        "            eval_set=[(X_vl, y_vl)],\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "            #verbose=False\n",
        "        )\n",
        "\n",
        "        val_preds = model.predict(X_vl)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    final_predictions[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# Overall score\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# Save submission\n",
        "final_predictions.to_csv(\"/content/drive/MyDrive/best_lgb_submission.csv\", index=False)\n",
        "# Save best parameters to pickle file\n",
        "with open(\"/content/drive/MyDrive/best_lgb_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_params_dict, f)\n",
        "\n",
        "# Save best parameters to PDF\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "pdf.cell(200, 10, txt=\"Best LGB Parameters per Target\", ln=True, align=\"C\")\n",
        "pdf.ln(10)\n",
        "\n",
        "for target, params in best_params_dict.items():\n",
        "    pdf.set_font(\"Arial\", 'B', size=12)\n",
        "    pdf.cell(200, 10, txt=f\"{target}:\", ln=True)\n",
        "    pdf.set_font(\"Arial\", size=11)\n",
        "    for key, value in params.items():\n",
        "        pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\n",
        "    pdf.ln(5)\n",
        "\n",
        "pdf.output(\"/content/drive/MyDrive/best_lgb_params.pdf\") \"\"\""
      ],
      "metadata": {
        "id": "ZTF_JYSdcZM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import lightgbm as lgb\n",
        "import pickle\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "dir_with_params= './'\n",
        "\n",
        "# === Load best parameters ===\n",
        "with open(dir_with_params+\"best_lgb_params.pkl\", \"rb\") as f:\n",
        "    best_params_dict = pickle.load(f)\n",
        "\n",
        "# === Define targets and their level mappings ===\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# === Prepare test set ===\n",
        "X_test = test_df.drop(columns=['PID', 'site'])\n",
        "test_ids = test_df['PID']\n",
        "\n",
        "# === Prediction containers ===\n",
        "LGB = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "\n",
        "# === Loop through each target ===\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Retraining for Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "\n",
        "    # === Prepare training data ===\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**best_params_dict[target])\n",
        "        model.fit(\n",
        "            X_tr, y_tr,\n",
        "            eval_set=[(X_vl, y_vl)],\n",
        "            callbacks=[lgb.early_stopping(stopping_rounds=50)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        val_preds = model.predict(X_vl)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    LGB[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# === Overall score ===\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# === Save predictions ===\n",
        "LGB.to_csv(\"/content/drive/MyDrive/Final_LGB_Retrained_Submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "sJasAfsPdP9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Define targets and corresponding level columns\n",
        "target_columns = ['Ca', 'P', 'K', 'N', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# Prepare test set\n",
        "X_test = test_df.drop(columns=['PID', 'site'])\n",
        "test_ids = test_df['PID']\n",
        "\n",
        "# Define Optuna objective function for Random Forest\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 20, 30),\n",
        "       # \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
        "       # \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
        "        #\"max_features\": trial.suggest_categorical(\"max_features\", [\"log2\", \"sqrt\", None]),\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1\n",
        "    }\n",
        "\n",
        "    model = RandomForestRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    return np.sqrt(mean_squared_error(y_val, preds))\n",
        "\n",
        "# Containers to track results\n",
        "final_predictions = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "best_params_dict = {}\n",
        "\n",
        "# Loop over each target variable\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Processing Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    # Split for tuning\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=stratify_vals, random_state=42\n",
        "    )\n",
        "\n",
        "    # Optuna tuning\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    for _ in tqdm(range(20), desc=f\"Tuning {target}\"):\n",
        "        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=1)\n",
        "\n",
        "    best_params = study.best_trial.params\n",
        "    print(f\"Best Parameters for {target}: {best_params}\")\n",
        "    best_params_dict[target] = best_params\n",
        "\n",
        "    # Cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = RandomForestRegressor(\n",
        "            **best_params,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        val_preds = model.predict(X_vl)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    final_predictions[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# Overall score\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# Save submission\n",
        "final_predictions.to_csv(\"/content/drive/MyDrive/submission_rf.csv\", index=False)\n",
        "# Save best parameters to pickle file\n",
        "with open(\"/content/drive/MyDrive/best_Rand_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_params_dict, f)\n",
        "\n",
        "# Save best parameters to PDF\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "pdf.cell(200, 10, txt=\"Best CatBoost Parameters per Target\", ln=True, align=\"C\")\n",
        "pdf.ln(10)\n",
        "\n",
        "for target, params in best_params_dict.items():\n",
        "    pdf.set_font(\"Arial\", 'B', size=12)\n",
        "    pdf.cell(200, 10, txt=f\"{target}:\", ln=True)\n",
        "    pdf.set_font(\"Arial\", size=11)\n",
        "    for key, value in params.items():\n",
        "        pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\n",
        "    pdf.ln(5)\n",
        "\n",
        "pdf.output(\"/content/drive/MyDrive/best_Rand_params.pdf\") \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "collapsed": true,
        "id": "u_qKKPU7damI",
        "outputId": "79a1662d-3f7d-4975-c328-820535dc2656"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.metrics import mean_squared_error\\nimport optuna\\nfrom tqdm import tqdm\\n\\n\\n# Define targets and corresponding level columns\\ntarget_columns = [\\'Ca\\', \\'P\\', \\'K\\', \\'N\\', \\'Mg\\', \\'S\\', \\'Fe\\', \\'Mn\\', \\'Zn\\', \\'Cu\\', \\'B\\']\\nlevel_columns = {target: f\"{target}_level\" for target in target_columns}\\n\\n# Prepare test set\\nX_test = test_df.drop(columns=[\\'PID\\', \\'site\\'])\\ntest_ids = test_df[\\'PID\\']\\n\\n# Define Optuna objective function for Random Forest\\ndef objective(trial, X_train, y_train, X_val, y_val):\\n    params = {\\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\\n        \"max_depth\": trial.suggest_int(\"max_depth\", 20, 30),\\n       # \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\\n       # \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\\n        #\"max_features\": trial.suggest_categorical(\"max_features\", [\"log2\", \"sqrt\", None]),\\n        \"random_state\": 42,\\n        \"n_jobs\": -1\\n    }\\n\\n    model = RandomForestRegressor(**params)\\n    model.fit(X_train, y_train)\\n    preds = model.predict(X_val)\\n    return np.sqrt(mean_squared_error(y_val, preds))\\n\\n# Containers to track results\\nfinal_predictions = pd.DataFrame({\\'PID\\': test_ids})\\ncv_scores = {}\\nbest_params_dict = {}\\n\\n# Loop over each target variable\\nfor target in target_columns:\\n    print(f\"\\n=== Processing Target: {target} ===\")\\n    level_col = level_columns[target]\\n    drop_cols = target_columns + list(level_columns.values()) + [\\'PID\\', \\'site\\']\\n    X = train_df.drop(columns=drop_cols)\\n    y = train_df[target]\\n    stratify_vals = train_df[level_col]\\n\\n    # Split for tuning\\n    X_train, X_val, y_train, y_val = train_test_split(\\n        X, y, test_size=0.2, stratify=stratify_vals, random_state=42\\n    )\\n\\n    # Optuna tuning\\n    study = optuna.create_study(direction=\"minimize\")\\n    for _ in tqdm(range(20), desc=f\"Tuning {target}\"):\\n        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=1)\\n\\n    best_params = study.best_trial.params\\n    print(f\"Best Parameters for {target}: {best_params}\")\\n    best_params_dict[target] = best_params\\n\\n    # Cross-validation\\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n    fold_rmses = []\\n    fold_preds = np.zeros(len(X_test))\\n\\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\\n        print(f\" Fold {fold}/5\")\\n        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\\n        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\\n\\n        model = RandomForestRegressor(\\n            **best_params,\\n            random_state=42,\\n            n_jobs=-1\\n        )\\n\\n        model.fit(X_tr, y_tr)\\n        val_preds = model.predict(X_vl)\\n        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\\n        fold_rmses.append(rmse)\\n        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\\n\\n        fold_preds += model.predict(X_test) / skf.n_splits\\n\\n    mean_rmse = np.mean(fold_rmses)\\n    cv_scores[target] = mean_rmse\\n    final_predictions[target] = fold_preds\\n    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\\n\\n# Overall score\\noverall_rmse = np.mean(list(cv_scores.values()))\\nprint(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")\\n\\n# Save submission\\nfinal_predictions.to_csv(\"/content/drive/MyDrive/submission_rf.csv\", index=False)\\n# Save best parameters to pickle file\\nwith open(\"/content/drive/MyDrive/best_Rand_params.pkl\", \"wb\") as f:\\n    pickle.dump(best_params_dict, f)\\n\\n# Save best parameters to PDF\\npdf = FPDF()\\npdf.add_page()\\npdf.set_font(\"Arial\", size=12)\\npdf.cell(200, 10, txt=\"Best CatBoost Parameters per Target\", ln=True, align=\"C\")\\npdf.ln(10)\\n\\nfor target, params in best_params_dict.items():\\n    pdf.set_font(\"Arial\", \\'B\\', size=12)\\n    pdf.cell(200, 10, txt=f\"{target}:\", ln=True)\\n    pdf.set_font(\"Arial\", size=11)\\n    for key, value in params.items():\\n        pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\\n    pdf.ln(5)\\n\\npdf.output(\"/content/drive/MyDrive/best_Rand_params.pdf\") '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Load best parameters ===\n",
        "dir_with_params= './'\n",
        "with open(dir_with_params+\"best_Rand_params.pkl\", \"rb\") as f:\n",
        "    best_params_dict = pickle.load(f)\n",
        "\n",
        "# === Define targets and level mappings ===\n",
        "target_columns = ['Ca', 'P', 'K', 'N', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "level_columns = {target: f\"{target}_level\" for target in target_columns}\n",
        "\n",
        "# === Prepare test set ===\n",
        "X_test = test_df.drop(columns=['PID', 'site'])\n",
        "test_ids = test_df['PID']\n",
        "\n",
        "# === Initialize output containers ===\n",
        "RF = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "\n",
        "# === Train and Predict for Each Target ===\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Retraining for Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "\n",
        "    drop_cols = target_columns + list(level_columns.values()) + ['PID', 'site']\n",
        "    X = train_df.drop(columns=drop_cols)\n",
        "    y = train_df[target]\n",
        "    stratify_vals = train_df[level_col]\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_rmses = []\n",
        "    fold_preds = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, stratify_vals), 1):\n",
        "        print(f\" Fold {fold}/5\")\n",
        "\n",
        "        X_tr, X_vl = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_vl = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model = RandomForestRegressor(\n",
        "            **best_params_dict[target],\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        val_preds = model.predict(X_vl)\n",
        "        rmse = np.sqrt(mean_squared_error(y_vl, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "        print(f\"  Fold {fold} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        fold_preds += model.predict(X_test) / skf.n_splits\n",
        "\n",
        "    mean_rmse = np.mean(fold_rmses)\n",
        "    cv_scores[target] = mean_rmse\n",
        "    RF[target] = fold_preds\n",
        "    print(f\" Mean CV RMSE for {target}: {mean_rmse:.4f}\")\n",
        "\n",
        "# === Save final predictions ===\n",
        "RF.to_csv(\"/content/drive/MyDrive/Final_RandomForest_Submission.csv\", index=False)\n",
        "\n",
        "# === Report overall score ===\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall CV RMSE across all targets: {overall_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "ikoh0DJueRgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "# Feature selection\n",
        "X = train_df.drop(columns=target_columns)\n",
        "y = train_df[target_columns]\n",
        "X_test = test_df.drop(columns=['PID',\"site\"])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stratified split by Ca_level\n",
        "X_train1, X_val1, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "   )\n",
        "model = MultiOutputRegressor(RandomForestRegressor(n_estimators=50, random_state=42))\n",
        "model.fit(X_train, y_train)\n",
        "# Predict on validation set\n",
        "predictions =model.predict(X_test)\n",
        "y_pred = model.predict(X_val.drop(columns=list(level_columns.values())))\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Split the predictions into separate columns\n",
        "N_pred =  test_predictions[:, 0]  # Predictions for N\n",
        "P_pred =  test_predictions[:, 1]  # Predictions for P\n",
        "K_pred =  test_predictions[:, 2]  # Predictions for K\n",
        "Ca_pred = test_predictions[:, 3]  # Predictions for Ca\n",
        "Mg_pred = test_predictions[:, 4]  # Predictions for Mg\n",
        "S_pred =  test_predictions[:, 5]  # Predictions for S\n",
        "Fe_pred = test_predictions[:, 6]  # Predictions for Fe\n",
        "Mn_pred = test_predictions[:, 7]  # Predictions for Mn\n",
        "Zn_pred = test_predictions[:, 8]  # Predictions for Zn\n",
        "Cu_pred = test_predictions[:, 9]  # Predictions for Cu\n",
        "B_pred =  test_predictions[:, 10]  # Predictions for B\n",
        "submissionF = pd.DataFrame({'PID': test_df['PID'],'pH':test_df['pH'],'lon':test_df['lon'],'N': N_pred, 'P': P_pred, 'K': K_pred, 'Ca': Ca_pred, 'Mg': Mg_pred, 'S': S_pred, 'Fe': Fe_pred, 'Mn': Mn_pred, 'Zn': Zn_pred, 'Cu': Cu_pred, 'B': B_pred})\n",
        "submissionF.loc[(submissionF['pH'] > 7.9) &(submissionF['pH'] <= 8.1)&(submissionF['Ca'] > 12500), 'Ca'] *= 1.4\n",
        "prob_points=submissionF\n",
        "prob_points=pd.read_csv('/content/drive/MyDrive/Prob_points.csv')\n",
        "#Dropped IDs which didn't affect the value of the lb score\n",
        "ids_to_drop = ['ID_mvSW55', 'ID_lJ5FgG','ID_MmPPW8','ID_wCVVRL','ID_IeSToR']\n",
        "prob_points = prob_points[~prob_points['PID'].isin(ids_to_drop)].reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "collapsed": true,
        "id": "aspyNzOZeYYv",
        "outputId": "9a2ca15e-39db-4bfb-9169-ed486db1f8bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-10-1791186287.py, line 34)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-10-1791186287.py\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    prob_points=submission.loc[(submission['pH'] > 7.9) &(submission['pH'] <= 8.1)&(submission['Ca'] > 12500), 'Ca'] *= 1.4\u001b[0m\n\u001b[0m                                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dfs = [CAT, LGB, XGB, RF]\n",
        "\n",
        "# Ensure all dataframes contain 'id' and the target columns\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "# Step 1: Sort and align columns, keeping only relevant columns\n",
        "aligned_dfs = []\n",
        "for df in dfs:\n",
        "    columns_to_keep = ['PID'] + [col for col in target_columns if col in df.columns]\n",
        "    aligned_dfs.append(df[columns_to_keep].copy())\n",
        "\n",
        "# Step 2: Merge all dataframes on 'id'\n",
        "from functools import reduce\n",
        "\n",
        "# Rename columns to avoid overlap (except for 'id')\n",
        "for i, df in enumerate(aligned_dfs):\n",
        "    df.columns = ['PID'] + [f\"{col}_df{i+1}\" for col in df.columns if col != 'PID']\n",
        "\n",
        "# Merge on 'id'\n",
        "merged_df = reduce(lambda left, right: pd.merge(left, right, on='PID', how='outer'), aligned_dfs)\n",
        "\n",
        "# Step 3: Compute average per target column\n",
        "result = pd.DataFrame()\n",
        "result['PID'] = merged_df['PID']\n",
        "\n",
        "for col in target_columns:\n",
        "    # Collect all columns corresponding to this target\n",
        "    value_columns = [c for c in merged_df.columns if c.startswith(col + '_df')]\n",
        "    # Row-wise mean ignoring NaNs\n",
        "    result[col] = merged_df[value_columns].mean(axis=1)\n",
        "\n",
        "# Final result is a dataframe with id and the average values\n",
        "result = submission"
      ],
      "metadata": {
        "id": "u3ZX-zNLfG0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "submission.set_index('PID', inplace=True)\n",
        "prob_points.set_index('PID', inplace=True)\n",
        "\n",
        "# Update only the 'Ca' column\n",
        "submission['Ca'].update(prob_points['Ca'])\n",
        "prob_points.reset_index(inplace=True)\n",
        "submission.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "PY-rCHvmgNFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: turn submission into a 3 column file that has the column PID, Nutrient, Value\n",
        "\n",
        "submission_melted = submission.melt(id_vars=['PID'], var_name='Nutrient', value_name='Available_Nutrients_in_ppm')\n",
        "submission_melted = submission_melted.sort_values('PID')\n",
        "submission_melted.head()"
      ],
      "metadata": {
        "id": "wWmp4Hi2gUjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: merge test_gap_df with submission_melted on PID and Nutrient\n",
        "nutrient_df = pd.merge(test_gap_df, submission_melted, on=['PID', 'Nutrient'], how='left')\n",
        "soil_depth = 20  # cm\n",
        "\n",
        "# Calculate the Available_Nutrients_in_kg_ha\n",
        "nutrient_df['Available_Nutrients_in_kg_ha'] = (nutrient_df['Available_Nutrients_in_ppm']\n",
        "                                               * soil_depth * nutrient_df['BulkDensity'] * 0.1)\n",
        "nutrient_df['Gap']=nutrient_df['Required']-nutrient_df['Available_Nutrients_in_kg_ha']\n",
        "nutrient_df['ID'] = nutrient_df['PID'] + \"_\" + nutrient_df['Nutrient']\n",
        "nutrient_df = nutrient_df[['ID', 'Gap']]"
      ],
      "metadata": {
        "id": "j85n_10JgVZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nutrient_df.to_csv('Overfit_ensemble.csv', index=False)"
      ],
      "metadata": {
        "id": "6tDyoP6pgZtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "94s63EfEgsWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}