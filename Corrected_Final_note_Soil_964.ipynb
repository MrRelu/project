{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW99OQLwi0wZoRUAWLXYwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrRelu/project/blob/main/Corrected_Final_note_Soil_964.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6wIx7w7Cn1o",
        "outputId": "392aa793-2888-472b-8948-07a4eb97d008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/muhammadqasimshabbir/amini-soil-prediction-challenge-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 518M/518M [00:07<00:00, 74.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "dataset_path = kagglehub.dataset_download('muhammadqasimshabbir/amini-soil-prediction-challenge-dataset')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna fpdf xgboost catboost optuna-integration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tz89tciOFeK2",
        "outputId": "b098e098-5b29-4593-8289-84ec3ced25e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting optuna-integration\n",
            "  Downloading optuna_integration-4.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna_integration-4.4.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=f2e33a5c64fb15ce6c0b0131beead3a2559c583b339af2a5d05bfd3178b14631\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf, colorlog, alembic, optuna, catboost, optuna-integration\n",
            "Successfully installed alembic-1.16.2 catboost-1.2.8 colorlog-6.9.0 fpdf-1.7.2 optuna-4.4.0 optuna-integration-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "DMzUM8ekDDb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_df = pd.read_csv(dataset_path+'/Train.csv')\n",
        "test_df = pd.read_csv(dataset_path+'/Test.csv')\n",
        "train_gap_df = pd.read_csv(dataset_path+'/Gap_Train.csv')\n",
        "test_gap_df1 = pd.read_csv(dataset_path+'/Gap_Test.csv')\n",
        "sample_submission = pd.read_csv(dataset_path+'/SampleSubmission.csv')\n",
        "sat8=pd.read_csv(dataset_path+'/LANDSAT8_data_updated.csv')\n",
        "test_gap_df = pd.merge(test_gap_df1, test_df[['PID', 'BulkDensity']], on='PID')"
      ],
      "metadata": {
        "id": "zuJ7iydWERVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: input missing values in train_df and test_df with the mean, only do it for columns that have missing values\n",
        "\n",
        "# Fill missing values with the mean for columns with missing values in train_df\n",
        "for column in train_df.columns:\n",
        "  if train_df[column].isnull().any():\n",
        "    train_df[column].fillna(train_df[column].mean(), inplace=True)\n",
        "\n",
        "# Fill missing values with the mean for columns with missing values in test_df\n",
        "for column in test_df.columns:\n",
        "  if test_df[column].isnull().any():\n",
        "    test_df[column].fillna(test_df[column].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "pmwchBgIE7EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from functools import reduce\n",
        "\n",
        "# Ensure datetime and extract components\n",
        "sat8['date'] = pd.to_datetime(sat8['date'])\n",
        "sat8['year'] = sat8['date'].dt.year\n",
        "sat8['month'] = sat8['date'].dt.month\n",
        "\n",
        "# Define value columns (exclude metadata)\n",
        "exclude_cols = ['date', 'month', 'year']\n",
        "value_cols = sat8.columns.difference(['PID'] + exclude_cols)\n",
        "\n",
        "# --- Aggregation: Entire dataset (2017–2019)\n",
        "avg_all = sat8.groupby(\"PID\")[value_cols].mean().add_suffix('_all')\n",
        "\n",
        "# --- Aggregation: 2018 and 2019 only\n",
        "avg_2018_2019 = sat8[sat8['year'].isin([2018, 2019])]\\\n",
        "    .groupby(\"PID\")[value_cols].mean().add_suffix('_2018_2019')\n",
        "\n",
        "# --- Aggregation: 2019 only\n",
        "avg_2019 = sat8[sat8['year'] == 2019]\\\n",
        "    .groupby(\"PID\")[value_cols].mean().add_suffix('_2019')\n",
        "\n",
        "# --- Southern Hemisphere Seasons for 2019\n",
        "# 1. Summer 2019 = Dec 2018 + Jan, Feb 2019\n",
        "summer_2019 = sat8[((sat8['year'] == 2019) & (sat8['month'].isin([1, 2]))) |\n",
        "                   ((sat8['year'] == 2018) & (sat8['month'] == 12))]\n",
        "avg_summer_2019 = summer_2019.groupby(\"PID\")[value_cols].mean().add_suffix('_summer2019')\n",
        "\n",
        "# 2. Autumn 2019 = Mar, Apr, May 2019\n",
        "autumn_2019 = sat8[(sat8['year'] == 2019) & (sat8['month'].isin([3, 4, 5]))]\n",
        "avg_autumn_2019 = autumn_2019.groupby(\"PID\")[value_cols].mean().add_suffix('_autumn2019')\n",
        "\n",
        "# 3. Winter 2019 = Jun, Jul, Aug 2019\n",
        "winter_2019 = sat8[(sat8['year'] == 2019) & (sat8['month'].isin([6, 7, 8]))]\n",
        "avg_winter_2019 = winter_2019.groupby(\"PID\")[value_cols].mean().add_suffix('_winter2019')\n",
        "\n",
        "# 4. Spring 2019 = Sep, Oct, Nov 2019\n",
        "spring_2019 = sat8[(sat8['year'] == 2019) & (sat8['month'].isin([9, 10, 11]))]\n",
        "avg_spring_2019 = spring_2019.groupby(\"PID\")[value_cols].mean().add_suffix('_spring2019')\n",
        "\n",
        "# --- Combine everything\n",
        "final_df = reduce(lambda left, right: left.join(right, how='outer'), [\n",
        "    avg_all,\n",
        "    avg_2018_2019,\n",
        "    avg_2019,\n",
        "    avg_summer_2019,\n",
        "    avg_autumn_2019,\n",
        "    avg_winter_2019,\n",
        "    avg_spring_2019\n",
        "]).reset_index()"
      ],
      "metadata": {
        "id": "r6_E6K1KEc5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge on PID\n",
        "train_merged_df = train_df.merge(final_df, on='PID', how='left')\n",
        "test_merged_df = test_df.merge(final_df, on='PID', how='left')"
      ],
      "metadata": {
        "id": "_ga71ODLFK47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" # === MOUNT GOOGLE DRIVE ===\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# === Set working paths ===\n",
        "base_dir = \"/content/drive/MyDrive/Extra_LonCV_XGBoost\"\n",
        "params_dir = f\"{base_dir}/best_params\"\n",
        "import os\n",
        "os.makedirs(params_dir, exist_ok=True)\n",
        "\n",
        "# === IMPORTS ===\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "import pickle\n",
        "from fpdf import FPDF\n",
        "\n",
        "# === Target & Level Columns ===\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "# === Test Set Prep ===\n",
        "X_test = test_merged_df.drop(columns=['PID', 'site', 'lat', 'lon', 'bio7', 'bio15', 'mdem'])\n",
        "test_ids = test_merged_df['PID']\n",
        "\n",
        "# === Reproducible Binning ===\n",
        "bin_path = f\"{params_dir}/lon_bin_edges.npy\"\n",
        "if os.path.exists(bin_path):\n",
        "    print(\"Loading previously saved longitude bin edges...\")\n",
        "    lon_bin_edges = np.load(bin_path)\n",
        "else:\n",
        "    print(\"Creating and saving longitude bin edges...\")\n",
        "    _, lon_bin_edges = pd.qcut(train_merged_df['lon'], q=5, retbins=True, duplicates='drop')\n",
        "    np.save(bin_path, lon_bin_edges)\n",
        "\n",
        "train_merged_df['lon_bin'] = pd.cut(train_merged_df['lon'], bins=lon_bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "# === Optuna Objective ===\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "        \"n_estimators\": 1000,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 11),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"random_state\": 42,\n",
        "        \"verbosity\": 0,\n",
        "         \"early_stopping_rounds\":20,\n",
        "    }\n",
        "    model = XGBRegressor(**params)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "\n",
        "              verbose=False)\n",
        "    preds = model.predict(X_val)\n",
        "    return np.sqrt(mean_squared_error(y_val, preds))\n",
        "\n",
        "# === Training Loop ===\n",
        "XGB = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "all_fold_params = {}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Processing Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "    drop_cols = target_columns  + ['PID', 'site', 'lat', 'lon', 'lon_bin', 'bio7', 'bio15', 'mdem']\n",
        "\n",
        "    X_full = train_merged_df.drop(columns=drop_cols)\n",
        "    y_full = train_merged_df[target]\n",
        "    fold_rmses = []\n",
        "    fold_params_list = {}\n",
        "    fold_preds_array = np.zeros((len(X_test), 5))  # for test set ensembling\n",
        "\n",
        "    for bin_val in range(5):\n",
        "        print(f\"Fold {bin_val}\")\n",
        "        val_mask = (train_merged_df['lon_bin'] == bin_val)\n",
        "        train_mask = ~val_mask\n",
        "\n",
        "        X_train, X_val = X_full[train_mask], X_full[val_mask]\n",
        "        y_train, y_val = y_full[train_mask], y_full[val_mask]\n",
        "\n",
        "        # === Hyperparameter Tuning ===\n",
        "        pruner = SuccessiveHalvingPruner()\n",
        "        study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=10)\n",
        "\n",
        "        best_params = study.best_trial.params\n",
        "        fold_params_list[f\"fold_{bin_val}\"] = best_params\n",
        "\n",
        "        model = XGBRegressor(\n",
        "            n_estimators=1000,\n",
        "            early_stopping_rounds=20,\n",
        "            **best_params\n",
        "        )\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "        # === Predict on val and accumulate RMSE\n",
        "        val_preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "\n",
        "        # === Predict on test\n",
        "        fold_preds_array[:, bin_val] = model.predict(X_test)\n",
        "\n",
        "        # === Save Feature Importance (only once)\n",
        "        if bin_val == 0:\n",
        "            importance_df = pd.DataFrame({\n",
        "                \"Feature\": X_train.columns,\n",
        "                \"Importance\": model.feature_importances_\n",
        "            }).sort_values(by=\"Importance\", ascending=False)\n",
        "            importance_df.to_csv(f\"{params_dir}/lon_feature_importance_{target}.csv\", index=False)\n",
        "\n",
        "        # === Save Pickle Params\n",
        "        with open(f\"{params_dir}/{target}_fold{bin_val}_params.pkl\", \"wb\") as f:\n",
        "            pickle.dump(best_params, f)\n",
        "\n",
        "        # === Save PDF Params\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.cell(200, 10, txt=f\"Best XGBoost Parameters for {target} - Fold {bin_val}\", ln=True, align=\"C\")\n",
        "        pdf.ln(10)\n",
        "        for key, value in best_params.items():\n",
        "            pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\n",
        "        pdf.output(f\"{params_dir}/{target}_fold{bin_val}_params.pdf\")\n",
        "\n",
        "    # === Ensemble predictions by averaging across folds\n",
        "    XGB[target] = fold_preds_array.mean(axis=1)\n",
        "\n",
        "    # === Save per-target fold predictions\n",
        "    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\n",
        "        f\"{params_dir}/{target}_test_fold_predictions.csv\", index=False\n",
        "    )\n",
        "\n",
        "    avg_rmse = np.mean(fold_rmses)\n",
        "    print(f\"Average RMSE for {target}: {avg_rmse:.4f}\")\n",
        "    cv_scores[target] = avg_rmse\n",
        "    all_fold_params[target] = fold_params_list\n",
        "\n",
        "# === Summary Output ===\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall Validation RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# === Save final outputs to Google Drive ===\n",
        "XGB.to_csv(f\"{base_dir}/LonCV_XGB_submission.csv\", index=False)\n",
        "\n",
        "with open(f\"{params_dir}/LonCV_all_fold_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_fold_params, f) \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "collapsed": true,
        "id": "K7TVfgg9F_-o",
        "outputId": "b0b640c9-34fb-461a-cd87-2cb5d70ef4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' # === MOUNT GOOGLE DRIVE ===\\n# from google.colab import drive\\n# drive.mount(\\'/content/drive\\')\\n\\n# === Set working paths ===\\nbase_dir = \"/content/drive/MyDrive/Extra_LonCV_XGBoost\"\\nparams_dir = f\"{base_dir}/best_params\"\\nimport os\\nos.makedirs(params_dir, exist_ok=True)\\n\\n# === IMPORTS ===\\nfrom xgboost import XGBRegressor\\nfrom sklearn.metrics import mean_squared_error\\nimport optuna\\nfrom optuna.pruners import SuccessiveHalvingPruner\\nimport pickle\\nfrom fpdf import FPDF\\n\\n# === Target & Level Columns ===\\ntarget_columns = [\\'N\\', \\'P\\', \\'K\\', \\'Ca\\', \\'Mg\\', \\'S\\', \\'Fe\\', \\'Mn\\', \\'Zn\\', \\'Cu\\', \\'B\\']\\nlevel_columns = {target: f\"{target}_level\" for target in target_columns}\\n\\n# === Test Set Prep ===\\nX_test = test_merged_df.drop(columns=[\\'PID\\', \\'site\\', \\'lat\\', \\'lon\\', \\'bio7\\', \\'bio15\\', \\'mdem\\'])\\ntest_ids = test_merged_df[\\'PID\\']\\n\\n# === Reproducible Binning ===\\nbin_path = f\"{params_dir}/lon_bin_edges.npy\"\\nif os.path.exists(bin_path):\\n    print(\"Loading previously saved longitude bin edges...\")\\n    lon_bin_edges = np.load(bin_path)\\nelse:\\n    print(\"Creating and saving longitude bin edges...\")\\n    _, lon_bin_edges = pd.qcut(train_merged_df[\\'lon\\'], q=5, retbins=True, duplicates=\\'drop\\')\\n    np.save(bin_path, lon_bin_edges)\\n\\ntrain_merged_df[\\'lon_bin\\'] = pd.cut(train_merged_df[\\'lon\\'], bins=lon_bin_edges, labels=False, include_lowest=True)\\n\\n# === Optuna Objective ===\\ndef objective(trial, X_train, y_train, X_val, y_val):\\n    params = {\\n        \"n_estimators\": 1000,\\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\\n        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 11),\\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\\n        \"random_state\": 42,\\n        \"verbosity\": 0,\\n         \"early_stopping_rounds\":20,\\n    }\\n    model = XGBRegressor(**params)\\n    model.fit(X_train, y_train,\\n              eval_set=[(X_val, y_val)],\\n              \\n              verbose=False)\\n    preds = model.predict(X_val)\\n    return np.sqrt(mean_squared_error(y_val, preds))\\n\\n# === Training Loop ===\\nfinal_predictions = pd.DataFrame({\\'PID\\': test_ids})\\ncv_scores = {}\\nall_fold_params = {}\\n\\nfor target in target_columns:\\n    print(f\"\\n=== Processing Target: {target} ===\")\\n    level_col = level_columns[target]\\n    drop_cols = target_columns + list(level_columns.values()) + [\\'PID\\', \\'site\\', \\'lat\\', \\'lon\\', \\'lon_bin\\', \\'bio7\\', \\'bio15\\', \\'mdem\\']\\n\\n    X_full = train_merged_df.drop(columns=drop_cols)\\n    y_full = train_merged_df[target]\\n    fold_rmses = []\\n    fold_params_list = {}\\n    fold_preds_array = np.zeros((len(X_test), 5))  # for test set ensembling\\n\\n    for bin_val in range(5):\\n        print(f\"Fold {bin_val}\")\\n        val_mask = (train_merged_df[\\'lon_bin\\'] == bin_val)\\n        train_mask = ~val_mask\\n\\n        X_train, X_val = X_full[train_mask], X_full[val_mask]\\n        y_train, y_val = y_full[train_mask], y_full[val_mask]\\n\\n        # === Hyperparameter Tuning ===\\n        pruner = SuccessiveHalvingPruner()\\n        study = optuna.create_study(direction=\"minimize\", pruner=pruner)\\n        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=10)\\n\\n        best_params = study.best_trial.params\\n        fold_params_list[f\"fold_{bin_val}\"] = best_params\\n\\n        model = XGBRegressor(\\n            n_estimators=1000,\\n            early_stopping_rounds=20,\\n            **best_params\\n        )\\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\\n\\n        # === Predict on val and accumulate RMSE\\n        val_preds = model.predict(X_val)\\n        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\\n        fold_rmses.append(rmse)\\n\\n        # === Predict on test\\n        fold_preds_array[:, bin_val] = model.predict(X_test)\\n\\n        # === Save Feature Importance (only once)\\n        if bin_val == 0:\\n            importance_df = pd.DataFrame({\\n                \"Feature\": X_train.columns,\\n                \"Importance\": model.feature_importances_\\n            }).sort_values(by=\"Importance\", ascending=False)\\n            importance_df.to_csv(f\"{params_dir}/lon_feature_importance_{target}.csv\", index=False)\\n\\n        # === Save Pickle Params\\n        with open(f\"{params_dir}/{target}_fold{bin_val}_params.pkl\", \"wb\") as f:\\n            pickle.dump(best_params, f)\\n\\n        # === Save PDF Params\\n        pdf = FPDF()\\n        pdf.add_page()\\n        pdf.set_font(\"Arial\", size=12)\\n        pdf.cell(200, 10, txt=f\"Best XGBoost Parameters for {target} - Fold {bin_val}\", ln=True, align=\"C\")\\n        pdf.ln(10)\\n        for key, value in best_params.items():\\n            pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\\n        pdf.output(f\"{params_dir}/{target}_fold{bin_val}_params.pdf\")\\n\\n    # === Ensemble predictions by averaging across folds\\n    final_predictions[target] = fold_preds_array.mean(axis=1)\\n\\n    # === Save per-target fold predictions\\n    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\\n        f\"{params_dir}/{target}_test_fold_predictions.csv\", index=False\\n    )\\n\\n    avg_rmse = np.mean(fold_rmses)\\n    print(f\"Average RMSE for {target}: {avg_rmse:.4f}\")\\n    cv_scores[target] = avg_rmse\\n    all_fold_params[target] = fold_params_list\\n\\n# === Summary Output ===\\noverall_rmse = np.mean(list(cv_scores.values()))\\nprint(f\"\\nOverall Validation RMSE across all targets: {overall_rmse:.4f}\")\\n\\n# === Save final outputs to Google Drive ===\\nfinal_predictions.to_csv(f\"{base_dir}/LonCV_XGB_submission.csv\", index=False)\\n\\nwith open(f\"{params_dir}/LonCV_all_fold_params.pkl\", \"wb\") as f:\\n    pickle.dump(all_fold_params, f) '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pickle\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# === Paths ===\n",
        "base_dir = \"/MyDrive/Extra_LonCV_XGBoost\"\n",
        "params_dir = f\"{base_dir}/best_params\"\n",
        "os.makedirs(params_dir, exist_ok=True)\n",
        "\n",
        "# === Target Columns ===\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "# === Prepare Test Set ===\n",
        "X_test = test_merged_df.drop(columns=['PID', 'site', 'lat', 'lon', 'bio7', 'bio15', 'mdem'])\n",
        "test_ids = test_merged_df['PID']\n",
        "\n",
        "# === Load Bin Edges and Create lon_bin ===\n",
        "lon_bin_edges = np.load(f\"{params_dir}/lon_bin_edges.npy\")\n",
        "train_merged_df['lon_bin'] = pd.cut(train_merged_df['lon'], bins=lon_bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "# === Inference Loop ===\n",
        "XGB = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Re-loading and Predicting Target: {target} ===\")\n",
        "\n",
        "    drop_cols = target_columns  + ['PID', 'site', 'lat', 'lon', 'lon_bin', 'bio7', 'bio15', 'mdem']\n",
        "    X_full = train_merged_df.drop(columns=drop_cols)\n",
        "    y_full = train_merged_df[target]\n",
        "\n",
        "    fold_preds_array = np.zeros((len(X_test), 5))  # for ensembling\n",
        "    fold_rmses = []\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"  Fold {fold}\")\n",
        "\n",
        "        # === Load Best Params ===\n",
        "        with open(f\"{params_dir}/{target}_fold{fold}_params.pkl\", \"rb\") as f:\n",
        "            best_params = pickle.load(f)\n",
        "\n",
        "        # === Setup Fold Split\n",
        "        val_mask = (train_merged_df['lon_bin'] == fold)\n",
        "        train_mask = ~val_mask\n",
        "\n",
        "        X_train, X_val = X_full[train_mask], X_full[val_mask]\n",
        "        y_train, y_val = y_full[train_mask], y_full[val_mask]\n",
        "\n",
        "        # === Re-train Model\n",
        "        model = XGBRegressor(\n",
        "            n_estimators=1000,\n",
        "            early_stopping_rounds=20,\n",
        "            **best_params\n",
        "        )\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "        # === Predict on Test\n",
        "        fold_preds_array[:, fold] = model.predict(X_test)\n",
        "\n",
        "        # === Optional: Evaluate again on validation\n",
        "        val_preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(np.mean((y_val - val_preds) ** 2))\n",
        "        fold_rmses.append(rmse)\n",
        "\n",
        "    # === Final Ensemble Prediction (mean of folds)\n",
        "    XGB[target] = fold_preds_array.mean(axis=1)\n",
        "\n",
        "    # === Save Fold Predictions\n",
        "    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\n",
        "        f\"{params_dir}/{target}_test_fold_predictions_reloaded.csv\", index=False\n",
        "    )\n",
        "\n",
        "    avg_rmse = np.mean(fold_rmses)\n",
        "    print(f\"  Reloaded RMSE for {target}: {avg_rmse:.4f}\")\n",
        "    cv_scores[target] = avg_rmse\n",
        "\n",
        "# === Save Final Submission\n",
        "XGB.to_csv(f\"{base_dir}/LonCV_XGB_submission_reload.csv\", index=False)\n",
        "\n",
        "# === Print Overall Score\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall RMSE on Reloaded Models: {overall_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "34BOM1vIGvmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import os\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "import pickle\n",
        "from fpdf import FPDF\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Set paths ===\n",
        "base_dir = \"/content/drive/MyDrive/Extra_LonCV_CatBoost\"\n",
        "params_dir = f\"{base_dir}/best_params\"\n",
        "os.makedirs(params_dir, exist_ok=True)\n",
        "\n",
        "# === Column definitions ===\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "# === Test set ===\n",
        "X_test = test_merged_df.drop(columns=['PID', 'site', 'lat', 'lon', 'bio7', 'bio15', 'mdem'])\n",
        "test_ids = test_merged_df['PID']\n",
        "\n",
        "# === Longitude binning ===\n",
        "bin_path = f\"{params_dir}/lon_bin_edges.npy\"\n",
        "if os.path.exists(bin_path):\n",
        "    lon_bin_edges = np.load(bin_path)\n",
        "else:\n",
        "    _, lon_bin_edges = pd.qcut(train_merged_df['lon'], q=5, retbins=True, duplicates='drop')\n",
        "    np.save(bin_path, lon_bin_edges)\n",
        "\n",
        "train_merged_df['lon_bin'] = pd.cut(train_merged_df['lon'], bins=lon_bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "# === Optuna objective ===\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "        \"iterations\": 700,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
        "        #\"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        #\"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
        "        \"random_seed\": 42,\n",
        "        \"early_stopping_rounds\": 20,\n",
        "        \"verbose\": False,\n",
        "        \"task_type\": \"GPU\"  # ✅ GPU Enabled\n",
        "    }\n",
        "    model = CatBoostRegressor(**params)\n",
        "    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
        "    preds = model.predict(X_val)\n",
        "    return np.sqrt(mean_squared_error(y_val, preds))\n",
        "\n",
        "# === Main loop ===\n",
        "CAT = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "all_fold_params = {}\n",
        "\n",
        "for target in tqdm(target_columns, desc=\"Processing Targets\"):\n",
        "    print(f\"\\n=== Target: {target} ===\")\n",
        "    drop_cols = target_columns  + ['PID', 'site', 'lat', 'lon', 'lon_bin', 'bio7', 'bio15', 'mdem']\n",
        "    X_full = train_merged_df.drop(columns=drop_cols)\n",
        "    y_full = train_merged_df[target]\n",
        "\n",
        "    fold_rmses = []\n",
        "    fold_params = {}\n",
        "    fold_preds_array = np.zeros((len(X_test), 5))\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"  Fold {fold}\")\n",
        "        val_mask = train_merged_df['lon_bin'] == fold\n",
        "        train_mask = ~val_mask\n",
        "\n",
        "        X_train, X_val = X_full[train_mask], X_full[val_mask]\n",
        "        y_train, y_val = y_full[train_mask], y_full[val_mask]\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", pruner=SuccessiveHalvingPruner())\n",
        "        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=10)\n",
        "\n",
        "        best_params = study.best_trial.params\n",
        "        fold_params[f\"fold_{fold}\"] = best_params\n",
        "\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=500,\n",
        "            early_stopping_rounds=20,\n",
        "            verbose=False,\n",
        "            task_type=\"GPU\",  # ✅ GPU ENABLED\n",
        "            **best_params\n",
        "        )\n",
        "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
        "\n",
        "        fold_preds_array[:, fold] = model.predict(X_test)\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, model.predict(X_val)))\n",
        "        fold_rmses.append(rmse)\n",
        "\n",
        "        # Save feature importance (only once)\n",
        "        if fold == 0:\n",
        "            importance_df = pd.DataFrame({\n",
        "                \"Feature\": X_train.columns,\n",
        "                \"Importance\": model.get_feature_importance()\n",
        "            }).sort_values(by=\"Importance\", ascending=False)\n",
        "            importance_df.to_csv(f\"{params_dir}/lon_feature_importance_{target}.csv\", index=False)\n",
        "\n",
        "        # Save params to pickle\n",
        "        with open(f\"{params_dir}/{target}_fold{fold}_params.pkl\", \"wb\") as f:\n",
        "            pickle.dump(best_params, f)\n",
        "\n",
        "        # Save PDF\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.cell(200, 10, txt=f\"Best CatBoost Params for {target} - Fold {fold}\", ln=True, align=\"C\")\n",
        "        pdf.ln(10)\n",
        "        for k, v in best_params.items():\n",
        "            pdf.cell(200, 8, txt=f\"{k}: {v}\", ln=True)\n",
        "        pdf.output(f\"{params_dir}/{target}_fold{fold}_params.pdf\")\n",
        "\n",
        "    # Save fold predictions\n",
        "    CAT[target] = fold_preds_array.mean(axis=1)\n",
        "    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\n",
        "        f\"{params_dir}/{target}_test_fold_predictions.csv\", index=False\n",
        "    )\n",
        "\n",
        "    avg_rmse = np.mean(fold_rmses)\n",
        "    print(f\"  Average RMSE: {avg_rmse:.4f}\")\n",
        "    cv_scores[target] = avg_rmse\n",
        "    all_fold_params[target] = fold_params\n",
        "\n",
        "# === Final outputs ===\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall Validation RMSE: {overall_rmse:.4f}\")\n",
        "\n",
        "CAT.to_csv(f\"{base_dir}/LonCV_CatBoost_submission.csv\", index=False)\n",
        "\n",
        "with open(f\"{params_dir}/LonCV_all_fold_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_fold_params, f) \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "collapsed": true,
        "id": "6HggctZ-G-xz",
        "outputId": "26dba193-a635-4f22-85d4-689041be1d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\nfrom catboost import CatBoostRegressor\\nfrom sklearn.metrics import mean_squared_error\\nimport optuna\\nfrom optuna.pruners import SuccessiveHalvingPruner\\nimport pickle\\nfrom fpdf import FPDF\\nfrom tqdm import tqdm\\n\\n# === Set paths ===\\nbase_dir = \"/content/drive/MyDrive/Extra_LonCV_CatBoost\"\\nparams_dir = f\"{base_dir}/best_params\"\\nos.makedirs(params_dir, exist_ok=True)\\n\\n# === Column definitions ===\\ntarget_columns = [\\'N\\', \\'P\\', \\'K\\', \\'Ca\\', \\'Mg\\', \\'S\\', \\'Fe\\', \\'Mn\\', \\'Zn\\', \\'Cu\\', \\'B\\']\\nlevel_columns = {target: f\"{target}_level\" for target in target_columns}\\n\\n# === Test set ===\\nX_test = test_merged_df.drop(columns=[\\'PID\\', \\'site\\', \\'lat\\', \\'lon\\', \\'bio7\\', \\'bio15\\', \\'mdem\\'])\\ntest_ids = test_merged_df[\\'PID\\']\\n\\n# === Longitude binning ===\\nbin_path = f\"{params_dir}/lon_bin_edges.npy\"\\nif os.path.exists(bin_path):\\n    lon_bin_edges = np.load(bin_path)\\nelse:\\n    _, lon_bin_edges = pd.qcut(train_merged_df[\\'lon\\'], q=5, retbins=True, duplicates=\\'drop\\')\\n    np.save(bin_path, lon_bin_edges)\\n\\ntrain_merged_df[\\'lon_bin\\'] = pd.cut(train_merged_df[\\'lon\\'], bins=lon_bin_edges, labels=False, include_lowest=True)\\n\\n# === Optuna objective ===\\ndef objective(trial, X_train, y_train, X_val, y_val):\\n    params = {\\n        \"iterations\": 700,\\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\\n        \"depth\": trial.suggest_int(\"depth\", 4, 10),\\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\\n        #\"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\\n        #\"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\\n        \"random_seed\": 42,\\n        \"early_stopping_rounds\": 20,\\n        \"verbose\": False,\\n        \"task_type\": \"GPU\"  # ✅ GPU Enabled\\n    }\\n    model = CatBoostRegressor(**params)\\n    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\\n    preds = model.predict(X_val)\\n    return np.sqrt(mean_squared_error(y_val, preds))\\n\\n# === Main loop ===\\nfinal_predictions = pd.DataFrame({\\'PID\\': test_ids})\\ncv_scores = {}\\nall_fold_params = {}\\n\\nfor target in tqdm(target_columns, desc=\"Processing Targets\"):\\n    print(f\"\\n=== Target: {target} ===\")\\n    drop_cols = target_columns + list(level_columns.values()) + [\\'PID\\', \\'site\\', \\'lat\\', \\'lon\\', \\'lon_bin\\', \\'bio7\\', \\'bio15\\', \\'mdem\\']\\n    X_full = train_merged_df.drop(columns=drop_cols)\\n    y_full = train_merged_df[target]\\n\\n    fold_rmses = []\\n    fold_params = {}\\n    fold_preds_array = np.zeros((len(X_test), 5))\\n\\n    for fold in range(5):\\n        print(f\"  Fold {fold}\")\\n        val_mask = train_merged_df[\\'lon_bin\\'] == fold\\n        train_mask = ~val_mask\\n\\n        X_train, X_val = X_full[train_mask], X_full[val_mask]\\n        y_train, y_val = y_full[train_mask], y_full[val_mask]\\n\\n        study = optuna.create_study(direction=\"minimize\", pruner=SuccessiveHalvingPruner())\\n        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=10)\\n\\n        best_params = study.best_trial.params\\n        fold_params[f\"fold_{fold}\"] = best_params\\n\\n        model = CatBoostRegressor(\\n            iterations=500,\\n            early_stopping_rounds=20,\\n            verbose=False,\\n            task_type=\"GPU\",  # ✅ GPU ENABLED\\n            **best_params\\n        )\\n        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\\n\\n        fold_preds_array[:, fold] = model.predict(X_test)\\n\\n        rmse = np.sqrt(mean_squared_error(y_val, model.predict(X_val)))\\n        fold_rmses.append(rmse)\\n\\n        # Save feature importance (only once)\\n        if fold == 0:\\n            importance_df = pd.DataFrame({\\n                \"Feature\": X_train.columns,\\n                \"Importance\": model.get_feature_importance()\\n            }).sort_values(by=\"Importance\", ascending=False)\\n            importance_df.to_csv(f\"{params_dir}/lon_feature_importance_{target}.csv\", index=False)\\n\\n        # Save params to pickle\\n        with open(f\"{params_dir}/{target}_fold{fold}_params.pkl\", \"wb\") as f:\\n            pickle.dump(best_params, f)\\n\\n        # Save PDF\\n        pdf = FPDF()\\n        pdf.add_page()\\n        pdf.set_font(\"Arial\", size=12)\\n        pdf.cell(200, 10, txt=f\"Best CatBoost Params for {target} - Fold {fold}\", ln=True, align=\"C\")\\n        pdf.ln(10)\\n        for k, v in best_params.items():\\n            pdf.cell(200, 8, txt=f\"{k}: {v}\", ln=True)\\n        pdf.output(f\"{params_dir}/{target}_fold{fold}_params.pdf\")\\n\\n    # Save fold predictions\\n    final_predictions[target] = fold_preds_array.mean(axis=1)\\n    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\\n        f\"{params_dir}/{target}_test_fold_predictions.csv\", index=False\\n    )\\n\\n    avg_rmse = np.mean(fold_rmses)\\n    print(f\"  Average RMSE: {avg_rmse:.4f}\")\\n    cv_scores[target] = avg_rmse\\n    all_fold_params[target] = fold_params\\n\\n# === Final outputs ===\\noverall_rmse = np.mean(list(cv_scores.values()))\\nprint(f\"\\nOverall Validation RMSE: {overall_rmse:.4f}\")\\n\\nfinal_predictions.to_csv(f\"{base_dir}/LonCV_CatBoost_submission.csv\", index=False)\\n\\nwith open(f\"{params_dir}/LonCV_all_fold_params.pkl\", \"wb\") as f:\\n    pickle.dump(all_fold_params, f) '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pickle\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# === Paths ===\n",
        "base_dir = \"/Extra_LonCV_CatBoost\"\n",
        "params_dir = f\"{base_dir}/best_params\"\n",
        "\n",
        "# === Target Columns ===\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "\n",
        "# === Test Set ===\n",
        "X_test = test_merged_df.drop(columns=['PID', 'site', 'lat', 'lon', 'bio7', 'bio15', 'mdem'])\n",
        "test_ids = test_merged_df['PID']\n",
        "\n",
        "# === Load bin edges and apply lon_bin ===\n",
        "lon_bin_edges = np.load(f\"{params_dir}/lon_bin_edges.npy\")\n",
        "train_merged_df['lon_bin'] = pd.cut(train_merged_df['lon'], bins=lon_bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "# === Reload, Retrain, Predict ===\n",
        "CAT = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Reloading & Predicting for Target: {target} ===\")\n",
        "\n",
        "    drop_cols = target_columns + ['PID', 'site', 'lat', 'lon', 'lon_bin', 'bio7', 'bio15', 'mdem']\n",
        "    X_full = train_merged_df.drop(columns=drop_cols)\n",
        "    y_full = train_merged_df[target]\n",
        "\n",
        "    fold_preds_array = np.zeros((len(X_test), 5))\n",
        "    fold_rmses = []\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"  Fold {fold}\")\n",
        "\n",
        "        # === Load best parameters\n",
        "        with open(f\"{params_dir}/{target}_fold{fold}_params.pkl\", \"rb\") as f:\n",
        "            best_params = pickle.load(f)\n",
        "\n",
        "        # Ensure fixed values are enforced\n",
        "        for k in ['iterations', 'early_stopping_rounds', 'verbose', 'task_type']:\n",
        "            best_params.pop(k, None)\n",
        "\n",
        "        # === Define train/val split\n",
        "        val_mask = train_merged_df['lon_bin'] == fold\n",
        "        train_mask = ~val_mask\n",
        "\n",
        "        X_train, X_val = X_full[train_mask], X_full[val_mask]\n",
        "        y_train, y_val = y_full[train_mask], y_full[val_mask]\n",
        "\n",
        "        # === Re-train model using loaded best params\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=700,\n",
        "            early_stopping_rounds=20,\n",
        "            verbose=False,\n",
        "            task_type=\"GPU\",\n",
        "            **best_params\n",
        "        )\n",
        "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
        "\n",
        "        # === Predict on test set\n",
        "        fold_preds_array[:, fold] = model.predict(X_test)\n",
        "\n",
        "        # === Optional: Evaluate on val set again\n",
        "        val_preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "\n",
        "    # === Ensemble prediction\n",
        "    CAT[target] = fold_preds_array.mean(axis=1)\n",
        "\n",
        "    # === Save fold-wise predictions\n",
        "    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\n",
        "        f\"{params_dir}/{target}_test_fold_predictions_reloaded.csv\", index=False\n",
        "    )\n",
        "\n",
        "    avg_rmse = np.mean(fold_rmses)\n",
        "    print(f\"  Reloaded RMSE for {target}: {avg_rmse:.4f}\")\n",
        "    cv_scores[target] = avg_rmse\n",
        "\n",
        "# === Save Final Submission\n",
        "CAT.to_csv(f\"{base_dir}/LonCV_CatBoost_submission_reload.csv\", index=False)\n",
        "\n",
        "# === Overall Validation RMSE\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall Reloaded RMSE: {overall_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "collapsed": true,
        "id": "7HxP4rcJHqjB",
        "outputId": "f6ec9228-59a5-474e-d48e-a010683875d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Reloading & Predicting for Target: N ===\n",
            "  Fold 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CatBoostError",
          "evalue": "catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-525063403.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         )\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# === Predict on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5874\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mCatBoostError\u001b[0m: catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 35: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "# === Set working paths ===\n",
        "base_dir = \"/content/drive/MyDrive/Extra_LonCV_LightGBM\"\n",
        "params_dir = f\"{base_dir}/best_params\"\n",
        "import os\n",
        "os.makedirs(params_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "import pickle\n",
        "from fpdf import FPDF\n",
        "\n",
        "# === Target & Level Columns ===\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "# === Test Set Prep ===\n",
        "X_test = test_merged_df.drop(columns=['PID', 'site', 'lat', 'lon', 'bio7', 'bio15', 'mdem'])\n",
        "test_ids = test_merged_df['PID']\n",
        "\n",
        "# === Reproducible Binning ===\n",
        "bin_path = f\"{params_dir}/lon_bin_edges.npy\"\n",
        "if os.path.exists(bin_path):\n",
        "    print(\"Loading previously saved longitude bin edges...\")\n",
        "    lon_bin_edges = np.load(bin_path)\n",
        "else:\n",
        "    print(\"Creating and saving longitude bin edges...\")\n",
        "    _, lon_bin_edges = pd.qcut(train_merged_df['lon'], q=5, retbins=True, duplicates='drop')\n",
        "    np.save(bin_path, lon_bin_edges)\n",
        "\n",
        "train_merged_df['lon_bin'] = pd.cut(train_merged_df['lon'], bins=lon_bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "# === Optuna Objective ===\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "        \"n_estimators\": 1000,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 11),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"random_state\": 42,\n",
        "        \"verbosity\": -1\n",
        "    }\n",
        "    model = LGBMRegressor(**params)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=\"rmse\",\n",
        "        callbacks=[lightgbm.early_stopping(stopping_rounds=20, verbose=False)]\n",
        "    )\n",
        "    preds = model.predict(X_val)\n",
        "    return np.sqrt(mean_squared_error(y_val, preds))\n",
        "\n",
        "# === Training Loop ===\n",
        "final_predictions = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "all_fold_params = {}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Processing Target: {target} ===\")\n",
        "    level_col = level_columns[target]\n",
        "    drop_cols = target_columns  + ['PID', 'site', 'lat', 'lon', 'lon_bin', 'bio7', 'bio15', 'mdem']\n",
        "\n",
        "    X_full = train_merged_df.drop(columns=drop_cols)\n",
        "    y_full = train_merged_df[target]\n",
        "    fold_rmses = []\n",
        "    fold_params_list = {}\n",
        "    fold_preds_array = np.zeros((len(X_test), 5))\n",
        "\n",
        "    for bin_val in range(5):\n",
        "        print(f\"Fold {bin_val}\")\n",
        "        val_mask = (train_merged_df['lon_bin'] == bin_val)\n",
        "        train_mask = ~val_mask\n",
        "\n",
        "        X_train, X_val = X_full[train_mask], X_full[val_mask]\n",
        "        y_train, y_val = y_full[train_mask], y_full[val_mask]\n",
        "\n",
        "        # === Hyperparameter Tuning ===\n",
        "        pruner = SuccessiveHalvingPruner()\n",
        "        study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
        "        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=10)\n",
        "\n",
        "        best_params = study.best_trial.params\n",
        "        fold_params_list[f\"fold_{bin_val}\"] = best_params\n",
        "\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=1000,\n",
        "            **best_params\n",
        "        )\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=\"rmse\",\n",
        "            callbacks=[lightgbm.early_stopping(stopping_rounds=20, verbose=False)]\n",
        "        )\n",
        "\n",
        "        val_preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "\n",
        "        fold_preds_array[:, bin_val] = model.predict(X_test)\n",
        "\n",
        "        if bin_val == 0:\n",
        "            importance_df = pd.DataFrame({\n",
        "                \"Feature\": X_train.columns,\n",
        "                \"Importance\": model.feature_importances_\n",
        "            }).sort_values(by=\"Importance\", ascending=False)\n",
        "            importance_df.to_csv(f\"{params_dir}/lon_feature_importance_{target}.csv\", index=False)\n",
        "\n",
        "        with open(f\"{params_dir}/{target}_fold{bin_val}_params.pkl\", \"wb\") as f:\n",
        "            pickle.dump(best_params, f)\n",
        "\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        pdf.cell(200, 10, txt=f\"Best LightGBM Parameters for {target} - Fold {bin_val}\", ln=True, align=\"C\")\n",
        "        pdf.ln(10)\n",
        "        for key, value in best_params.items():\n",
        "            pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\n",
        "        pdf.output(f\"{params_dir}/{target}_fold{bin_val}_params.pdf\")\n",
        "\n",
        "    final_predictions[target] = fold_preds_array.mean(axis=1)\n",
        "\n",
        "    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\n",
        "        f\"{params_dir}/{target}_test_fold_predictions.csv\", index=False\n",
        "    )\n",
        "\n",
        "    avg_rmse = np.mean(fold_rmses)\n",
        "    print(f\"Average RMSE for {target}: {avg_rmse:.4f}\")\n",
        "    cv_scores[target] = avg_rmse\n",
        "    all_fold_params[target] = fold_params_list\n",
        "\n",
        "# === Summary Output ===\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall Validation RMSE across all targets: {overall_rmse:.4f}\")\n",
        "\n",
        "# === Save final outputs to Google Drive ===\n",
        "final_predictions.to_csv(f\"{base_dir}/LonCV_LightGBM_submission.csv\", index=False)\n",
        "\n",
        "with open(f\"{params_dir}/LonCV_all_fold_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(all_fold_params, f) \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "collapsed": true,
        "id": "ujV_qPJyH3rv",
        "outputId": "99d883d9-124d-4121-99a9-1ea3cd31e928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# === Set working paths ===\\nbase_dir = \"/content/drive/MyDrive/Extra_LonCV_LightGBM\"\\nparams_dir = f\"{base_dir}/best_params\"\\nimport os\\nos.makedirs(params_dir, exist_ok=True)\\n\\n\\nfrom lightgbm import LGBMRegressor\\nimport lightgbm\\nfrom sklearn.metrics import mean_squared_error\\nimport optuna\\nfrom optuna.pruners import SuccessiveHalvingPruner\\nimport pickle\\nfrom fpdf import FPDF\\n\\n# === Target & Level Columns ===\\ntarget_columns = [\\'N\\', \\'P\\', \\'K\\', \\'Ca\\', \\'Mg\\', \\'S\\', \\'Fe\\', \\'Mn\\', \\'Zn\\', \\'Cu\\', \\'B\\']\\n\\n# === Test Set Prep ===\\nX_test = test_merged_df.drop(columns=[\\'PID\\', \\'site\\', \\'lat\\', \\'lon\\', \\'bio7\\', \\'bio15\\', \\'mdem\\'])\\ntest_ids = test_merged_df[\\'PID\\']\\n\\n# === Reproducible Binning ===\\nbin_path = f\"{params_dir}/lon_bin_edges.npy\"\\nif os.path.exists(bin_path):\\n    print(\"Loading previously saved longitude bin edges...\")\\n    lon_bin_edges = np.load(bin_path)\\nelse:\\n    print(\"Creating and saving longitude bin edges...\")\\n    _, lon_bin_edges = pd.qcut(train_merged_df[\\'lon\\'], q=5, retbins=True, duplicates=\\'drop\\')\\n    np.save(bin_path, lon_bin_edges)\\n\\ntrain_merged_df[\\'lon_bin\\'] = pd.cut(train_merged_df[\\'lon\\'], bins=lon_bin_edges, labels=False, include_lowest=True)\\n\\n# === Optuna Objective ===\\ndef objective(trial, X_train, y_train, X_val, y_val):\\n    params = {\\n        \"n_estimators\": 1000,\\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\\n        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 11),\\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),\\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\\n        \"random_state\": 42,\\n        \"verbosity\": -1\\n    }\\n    model = LGBMRegressor(**params)\\n    model.fit(\\n        X_train, y_train,\\n        eval_set=[(X_val, y_val)],\\n        eval_metric=\"rmse\",\\n        callbacks=[lightgbm.early_stopping(stopping_rounds=20, verbose=False)]\\n    )\\n    preds = model.predict(X_val)\\n    return np.sqrt(mean_squared_error(y_val, preds))\\n\\n# === Training Loop ===\\nfinal_predictions = pd.DataFrame({\\'PID\\': test_ids})\\ncv_scores = {}\\nall_fold_params = {}\\n\\nfor target in target_columns:\\n    print(f\"\\n=== Processing Target: {target} ===\")\\n    level_col = level_columns[target]\\n    drop_cols = target_columns  + [\\'PID\\', \\'site\\', \\'lat\\', \\'lon\\', \\'lon_bin\\', \\'bio7\\', \\'bio15\\', \\'mdem\\']\\n\\n    X_full = train_merged_df.drop(columns=drop_cols)\\n    y_full = train_merged_df[target]\\n    fold_rmses = []\\n    fold_params_list = {}\\n    fold_preds_array = np.zeros((len(X_test), 5))\\n\\n    for bin_val in range(5):\\n        print(f\"Fold {bin_val}\")\\n        val_mask = (train_merged_df[\\'lon_bin\\'] == bin_val)\\n        train_mask = ~val_mask\\n\\n        X_train, X_val = X_full[train_mask], X_full[val_mask]\\n        y_train, y_val = y_full[train_mask], y_full[val_mask]\\n\\n        # === Hyperparameter Tuning ===\\n        pruner = SuccessiveHalvingPruner()\\n        study = optuna.create_study(direction=\"minimize\", pruner=pruner)\\n        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=10)\\n\\n        best_params = study.best_trial.params\\n        fold_params_list[f\"fold_{bin_val}\"] = best_params\\n\\n        model = LGBMRegressor(\\n            n_estimators=1000,\\n            **best_params\\n        )\\n        model.fit(\\n            X_train, y_train,\\n            eval_set=[(X_val, y_val)],\\n            eval_metric=\"rmse\",\\n            callbacks=[lightgbm.early_stopping(stopping_rounds=20, verbose=False)]\\n        )\\n\\n        val_preds = model.predict(X_val)\\n        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\\n        fold_rmses.append(rmse)\\n\\n        fold_preds_array[:, bin_val] = model.predict(X_test)\\n\\n        if bin_val == 0:\\n            importance_df = pd.DataFrame({\\n                \"Feature\": X_train.columns,\\n                \"Importance\": model.feature_importances_\\n            }).sort_values(by=\"Importance\", ascending=False)\\n            importance_df.to_csv(f\"{params_dir}/lon_feature_importance_{target}.csv\", index=False)\\n\\n        with open(f\"{params_dir}/{target}_fold{bin_val}_params.pkl\", \"wb\") as f:\\n            pickle.dump(best_params, f)\\n\\n        pdf = FPDF()\\n        pdf.add_page()\\n        pdf.set_font(\"Arial\", size=12)\\n        pdf.cell(200, 10, txt=f\"Best LightGBM Parameters for {target} - Fold {bin_val}\", ln=True, align=\"C\")\\n        pdf.ln(10)\\n        for key, value in best_params.items():\\n            pdf.cell(200, 8, txt=f\"{key}: {value}\", ln=True)\\n        pdf.output(f\"{params_dir}/{target}_fold{bin_val}_params.pdf\")\\n\\n    final_predictions[target] = fold_preds_array.mean(axis=1)\\n\\n    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\\n        f\"{params_dir}/{target}_test_fold_predictions.csv\", index=False\\n    )\\n\\n    avg_rmse = np.mean(fold_rmses)\\n    print(f\"Average RMSE for {target}: {avg_rmse:.4f}\")\\n    cv_scores[target] = avg_rmse\\n    all_fold_params[target] = fold_params_list\\n\\n# === Summary Output ===\\noverall_rmse = np.mean(list(cv_scores.values()))\\nprint(f\"\\nOverall Validation RMSE across all targets: {overall_rmse:.4f}\")\\n\\n# === Save final outputs to Google Drive ===\\nfinal_predictions.to_csv(f\"{base_dir}/LonCV_LightGBM_submission.csv\", index=False)\\n\\nwith open(f\"{params_dir}/LonCV_all_fold_params.pkl\", \"wb\") as f:\\n    pickle.dump(all_fold_params, f) '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pickle\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# === Set working paths ===\n",
        "base_dir = \"/Extra_LonCV_LightGBM\"\n",
        "params_dir = f\"{base_dir}/best_params\"\n",
        "\n",
        "# === Target Columns ===\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "# === Test Set ===\n",
        "X_test = test_merged_df.drop(columns=['PID', 'site', 'lat', 'lon', 'bio7', 'bio15', 'mdem'])\n",
        "test_ids = test_merged_df['PID']\n",
        "\n",
        "# === Load bin edges and assign lon_bin ===\n",
        "lon_bin_edges = np.load(f\"{params_dir}/lon_bin_edges.npy\")\n",
        "train_merged_df['lon_bin'] = pd.cut(train_merged_df['lon'], bins=lon_bin_edges, labels=False, include_lowest=True)\n",
        "\n",
        "# === Inference Loop ===\n",
        "LGB = pd.DataFrame({'PID': test_ids})\n",
        "cv_scores = {}\n",
        "\n",
        "for target in target_columns:\n",
        "    print(f\"\\n=== Reloading & Predicting Target: {target} ===\")\n",
        "\n",
        "    drop_cols = target_columns  + ['PID', 'site', 'lat', 'lon', 'lon_bin', 'bio7', 'bio15', 'mdem']\n",
        "    X_full = train_merged_df.drop(columns=drop_cols)\n",
        "    y_full = train_merged_df[target]\n",
        "\n",
        "    fold_preds_array = np.zeros((len(X_test), 5))\n",
        "    fold_rmses = []\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"  Fold {fold}\")\n",
        "\n",
        "        # === Load best params\n",
        "        with open(f\"{params_dir}/{target}_fold{fold}_params.pkl\", \"rb\") as f:\n",
        "            best_params = pickle.load(f)\n",
        "\n",
        "        # Remove keys you want to fix manually\n",
        "        for k in ['n_estimators', 'verbosity']:\n",
        "            best_params.pop(k, None)\n",
        "\n",
        "        # === Split by fold\n",
        "        val_mask = train_merged_df['lon_bin'] == fold\n",
        "        train_mask = ~val_mask\n",
        "\n",
        "        X_train, X_val = X_full[train_mask], X_full[val_mask]\n",
        "        y_train, y_val = y_full[train_mask], y_full[val_mask]\n",
        "\n",
        "        # === Retrain\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=1000,\n",
        "            verbosity=-1,\n",
        "            **best_params\n",
        "        )\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            eval_metric=\"rmse\",\n",
        "            callbacks=[lightgbm.early_stopping(stopping_rounds=20, verbose=False)]\n",
        "        )\n",
        "\n",
        "        # === Predict test\n",
        "        fold_preds_array[:, fold] = model.predict(X_test)\n",
        "\n",
        "        # === Optional: Validate\n",
        "        val_preds = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
        "        fold_rmses.append(rmse)\n",
        "\n",
        "    # === Ensemble predictions\n",
        "    LGB[target] = fold_preds_array.mean(axis=1)\n",
        "\n",
        "    # === Save fold-wise predictions\n",
        "    pd.DataFrame(fold_preds_array, columns=[f\"{target}_fold{i}\" for i in range(5)]).to_csv(\n",
        "        f\"{params_dir}/{target}_test_fold_predictions_reloaded.csv\", index=False\n",
        "    )\n",
        "\n",
        "    avg_rmse = np.mean(fold_rmses)\n",
        "    print(f\"  Reloaded RMSE: {avg_rmse:.4f}\")\n",
        "    cv_scores[target] = avg_rmse\n",
        "\n",
        "# === Save Submission\n",
        "LGB.to_csv(f\"{base_dir}/LonCV_LightGBM_submission_reload.csv\", index=False)\n",
        "\n",
        "# === Final Summary\n",
        "overall_rmse = np.mean(list(cv_scores.values()))\n",
        "print(f\"\\nOverall Reloaded RMSE: {overall_rmse:.4f}\")"
      ],
      "metadata": {
        "id": "s7EEQschJbmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LGB=pd.read_csv('/content/drive/MyDrive/Extra_LonCV_LightGBM/LonCV_LightGBM_submission.csv')\n",
        "XGB=pd.read_csv('/content/drive/MyDrive/Extra_LonCV_XGBoost/LonCV_XGB_submission.csv')\n",
        "CAT=pd.read_csv('/content/drive/MyDrive/Extra_LonCV_CatBoost/LonCV_CatBoost_submission.csv') \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "collapsed": true,
        "id": "eIVLYT0NJsVa",
        "outputId": "d6409a56-4edd-42a8-b4fa-732b7388c29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" LGB=pd.read_csv('/content/drive/MyDrive/Extra_LonCV_LightGBM/LonCV_LightGBM_submission.csv')\\nXGB=pd.read_csv('/content/drive/MyDrive/Extra_LonCV_XGBoost/LonCV_XGB_submission.csv')\\nCAT=pd.read_csv('/content/drive/MyDrive/Extra_LonCV_CatBoost/LonCV_CatBoost_submission.csv') \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Post Processing\n",
        "To cover underestimated points in test_preds"
      ],
      "metadata": {
        "id": "oisFiCuFQac9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "# Feature selection\n",
        "X = train_df.drop(columns=target_columns)\n",
        "y = train_df[target_columns]\n",
        "X_test = test_df.drop(columns=['PID',\"site\"])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stratified split by Ca_level\n",
        "X_train1, X_val1, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "   )\n",
        "model = MultiOutputRegressor(RandomForestRegressor(n_estimators=50, random_state=42))\n",
        "model.fit(X_train, y_train)\n",
        "# Predict on validation set\n",
        "predictions =model.predict(X_test)\n",
        "y_pred = model.predict(X_val.drop(columns=list(level_columns.values())))\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Split the predictions into separate columns\n",
        "N_pred =  test_predictions[:, 0]  # Predictions for N\n",
        "P_pred =  test_predictions[:, 1]  # Predictions for P\n",
        "K_pred =  test_predictions[:, 2]  # Predictions for K\n",
        "Ca_pred = test_predictions[:, 3]  # Predictions for Ca\n",
        "Mg_pred = test_predictions[:, 4]  # Predictions for Mg\n",
        "S_pred =  test_predictions[:, 5]  # Predictions for S\n",
        "Fe_pred = test_predictions[:, 6]  # Predictions for Fe\n",
        "Mn_pred = test_predictions[:, 7]  # Predictions for Mn\n",
        "Zn_pred = test_predictions[:, 8]  # Predictions for Zn\n",
        "Cu_pred = test_predictions[:, 9]  # Predictions for Cu\n",
        "B_pred =  test_predictions[:, 10]  # Predictions for B\n",
        "Fsubmission = pd.DataFrame({'PID': test_df['PID'],'pH':test_df['pH'],'lon':test_df['lon'],'N': N_pred, 'P': P_pred, 'K': K_pred, 'Ca': Ca_pred, 'Mg': Mg_pred, 'S': S_pred, 'Fe': Fe_pred, 'Mn': Mn_pred, 'Zn': Zn_pred, 'Cu': Cu_pred, 'B': B_pred})\n",
        "Fsubmission.loc[(Fsubmission['pH'] > 7.9) &(Fsubmission['pH'] <= 8.1)&(Fsubmission['Ca'] > 12500), 'Ca'] *= 1.4\n",
        "prob_points=Fsubmission\n",
        "prob_points=pd.read_csv('/content/drive/MyDrive/Prob_points.csv')\n",
        "#Dropped IDs which didn't affect the value of the lb score\n",
        "ids_to_drop = ['ID_mvSW55', 'ID_lJ5FgG','ID_MmPPW8','ID_wCVVRL','ID_IeSToR']\n",
        "prob_points = prob_points[~prob_points['PID'].isin(ids_to_drop)].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "vdcM6FWlMibj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dfs = [CAT, LGB, XGB]\n",
        "\n",
        "# Ensure all dataframes contain 'id' and the target columns\n",
        "target_columns = ['N', 'P', 'K', 'Ca', 'Mg', 'S', 'Fe', 'Mn', 'Zn', 'Cu', 'B']\n",
        "\n",
        "# Step 1: Sort and align columns, keeping only relevant columns\n",
        "aligned_dfs = []\n",
        "for df in dfs:\n",
        "    columns_to_keep = ['PID'] + [col for col in target_columns if col in df.columns]\n",
        "    aligned_dfs.append(df[columns_to_keep].copy())\n",
        "\n",
        "# Step 2: Merge all dataframes on 'id'\n",
        "from functools import reduce\n",
        "\n",
        "# Rename columns to avoid overlap (except for 'id')\n",
        "for i, df in enumerate(aligned_dfs):\n",
        "    df.columns = ['PID'] + [f\"{col}_df{i+1}\" for col in df.columns if col != 'PID']\n",
        "\n",
        "# Merge on 'id'\n",
        "merged_df = reduce(lambda left, right: pd.merge(left, right, on='PID', how='outer'), aligned_dfs)\n",
        "\n",
        "# Step 3: Compute average per target column\n",
        "result = pd.DataFrame()\n",
        "result['PID'] = merged_df['PID']\n",
        "\n",
        "for col in target_columns:\n",
        "    # Collect all columns corresponding to this target\n",
        "    value_columns = [c for c in merged_df.columns if c.startswith(col + '_df')]\n",
        "    # Row-wise mean ignoring NaNs\n",
        "    result[col] = merged_df[value_columns].mean(axis=1)\n",
        "\n",
        "# Final result is a dataframe with id and the average values\n",
        "result = submission"
      ],
      "metadata": {
        "id": "LOrzlUeTLOW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "submission.set_index('PID', inplace=True)\n",
        "prob_points.set_index('PID', inplace=True)\n",
        "\n",
        "# Update only the 'Ca' column\n",
        "submission['Ca'].update(prob_points['Ca'])\n",
        "prob_points.reset_index(inplace=True)\n",
        "submission.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "78fVw0PQL5mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: turn submission into a 3 column file that has the column PID, Nutrient, Value\n",
        "\n",
        "submission_melted = submission.melt(id_vars=['PID'], var_name='Nutrient', value_name='Available_Nutrients_in_ppm')\n",
        "submission_melted = submission_melted.sort_values('PID')\n",
        "submission_melted.head()"
      ],
      "metadata": {
        "id": "b_DcTrMXL0uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: merge test_gap_df with submission_melted on PID and Nutrient\n",
        "nutrient_df = pd.merge(test_gap_df, submission_melted, on=['PID', 'Nutrient'], how='left')\n",
        "soil_depth = 20  # cm\n",
        "\n",
        "# Calculate the Available_Nutrients_in_kg_ha\n",
        "nutrient_df['Available_Nutrients_in_kg_ha'] = (nutrient_df['Available_Nutrients_in_ppm']\n",
        "                                               * soil_depth * nutrient_df['BulkDensity'] * 0.1)\n",
        "nutrient_df['Gap']=nutrient_df['Required']-nutrient_df['Available_Nutrients_in_kg_ha']\n",
        "nutrient_df['ID'] = nutrient_df['PID'] + \"_\" + nutrient_df['Nutrient']\n",
        "nutrient_df = nutrient_df[['ID', 'Gap']]"
      ],
      "metadata": {
        "id": "mvMTAW5NL7Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nutrient_df.to_csv('Safe_ensemble.csv', index=False)"
      ],
      "metadata": {
        "id": "_pBzaluVMPn_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}